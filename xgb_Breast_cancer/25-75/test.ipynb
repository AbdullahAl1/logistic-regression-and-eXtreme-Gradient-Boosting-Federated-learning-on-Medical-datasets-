{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-05-03 14:28:27,330 | 3147490696.py:55 | Loading partition...\n",
      "INFO flwr 2024-05-03 14:28:48,337 | 3147490696.py:65 | Reformatting data...\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "from typing import Union\n",
    "from logging import INFO\n",
    "from datasets import Dataset, DatasetDict\n",
    "import xgboost as xgb\n",
    "\n",
    "import flwr as fl\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common.logger import log\n",
    "from flwr.common import (\n",
    "    Code,\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    GetParametersIns,\n",
    "    GetParametersRes,\n",
    "    Parameters,\n",
    "    Status,\n",
    ")\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# Define data partitioning related functions\n",
    "def train_test_split(partition: Dataset, test_fraction: float, seed: int):\n",
    "    \"\"\"Split the data into train and validation set given split rate.\"\"\"\n",
    "    train_test = partition.train_test_split(test_size=test_fraction, seed=seed)\n",
    "    partition_train = train_test[\"train\"]\n",
    "    partition_test = train_test[\"test\"]\n",
    "\n",
    "    num_train = len(partition_train)\n",
    "    num_test = len(partition_test)\n",
    "\n",
    "    return partition_train, partition_test, num_train, num_test\n",
    "\n",
    "\n",
    "def transform_dataset_to_dmatrix(data: Union[Dataset, DatasetDict]) -> xgb.core.DMatrix:\n",
    "    \"\"\"Transform dataset to DMatrix format for xgboost.\"\"\"\n",
    "    x = data[\"inputs\"]\n",
    "    y = data[\"label\"]\n",
    "    new_data = xgb.DMatrix(x, label=y)\n",
    "    return new_data\n",
    "\n",
    "\n",
    "# Load (HIGGS) dataset and conduct partitioning\n",
    "# We use a small subset (num_partitions=30) of the dataset for demonstration to speed up the data loading process.\n",
    "partitioner = IidPartitioner(num_partitions=30)\n",
    "fds = FederatedDataset(dataset=\"jxie/higgs\", partitioners={\"train\": partitioner})\n",
    "\n",
    "# Load the partition for this `partition_id`\n",
    "log(INFO, \"Loading partition...\")\n",
    "partition = fds.load_partition(node_id=0, split=\"train\")\n",
    "partition.set_format(\"numpy\")\n",
    "\n",
    "# Train/test splitting\n",
    "train_data, valid_data, num_train, num_val = train_test_split(\n",
    "    partition, test_fraction=0.2, seed=42\n",
    ")\n",
    "import numpy as np\n",
    "# Reformat data to DMatrix for xgboost\n",
    "log(INFO, \"Reformatting data...\")\n",
    "train_dmatrix = transform_dataset_to_dmatrix(train_data)\n",
    "valid_dmatrix = transform_dataset_to_dmatrix(valid_data)\n",
    "\n",
    "# Hyper-parameters for xgboost training\n",
    "num_local_round = 1\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.1,  # Learning rate\n",
    "    \"max_depth\": 8,\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"nthread\": 16,\n",
    "    \"num_parallel_tree\": 1,\n",
    "    \"subsample\": 1,\n",
    "    \"tree_method\": \"hist\",\n",
    "}\n",
    "from sklearn.metrics import accuracy_score\n",
    "predictions = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-05-03 14:38:57,101 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)\n",
      "DEBUG flwr 2024-05-03 14:38:57,114 | connection.py:55 | ChannelConnectivity.IDLE\n",
      "DEBUG flwr 2024-05-03 14:38:57,116 | connection.py:55 | ChannelConnectivity.READY\n",
      "INFO flwr 2024-05-03 14:38:57,119 | 894417073.py:33 | Start training at round 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidate-error:0.39006\ttrain-error:0.38980\n",
      "[0.50720346 0.54078406 0.5326804  ... 0.5407039  0.5270872  0.54282004]\n",
      "[0]\tvalid-error:0.39005714285714288\n",
      "[0.47250575 0.56874794 0.54889095 ... 0.5443925  0.5141186  0.58131874]\n",
      "[2]\tvalid-error:0.31319999999999998\n",
      "[0.4390707  0.59106904 0.568166   ... 0.5377155  0.50377566 0.62653947]\n",
      "[4]\tvalid-error:0.29881428571428570\n",
      "[0.40488988 0.58968246 0.5863348  ... 0.5465034  0.49662665 0.65756816]\n",
      "[6]\tvalid-error:0.29345714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-03 14:38:58,104 | connection.py:220 | gRPC channel closed\n",
      "INFO flwr 2024-05-03 14:38:58,106 | app.py:398 | Disconnect and shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38830832 0.5818097  0.6075606  ... 0.5509021  0.4885172  0.68229896]\n",
      "[8]\tvalid-error:0.29018571428571427\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Flower client\n",
    "class XgbClient(fl.client.Client):\n",
    "    def __init__(self):\n",
    "        self.bst = None\n",
    "        self.config = None\n",
    "\n",
    "    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:\n",
    "        _ = (self, ins)\n",
    "        return GetParametersRes(\n",
    "            status=Status(\n",
    "                code=Code.OK,\n",
    "                message=\"OK\",\n",
    "            ),\n",
    "            parameters=Parameters(tensor_type=\"\", tensors=[]),\n",
    "        )\n",
    "\n",
    "    def _local_boost(self):\n",
    "        # Update trees based on local training data.\n",
    "        for i in range(num_local_round):\n",
    "            self.bst.update(train_dmatrix, self.bst.num_boosted_rounds())\n",
    "\n",
    "        # Extract the last N=num_local_round trees for sever aggregation\n",
    "        bst = self.bst[\n",
    "            self.bst.num_boosted_rounds()\n",
    "            - num_local_round : self.bst.num_boosted_rounds()\n",
    "        ]\n",
    "\n",
    "        return bst\n",
    "\n",
    "    def fit(self, ins: FitIns) -> FitRes:\n",
    "        if not self.bst:\n",
    "            # First round local training\n",
    "            log(INFO, \"Start training at round 1\")\n",
    "            bst = xgb.train(\n",
    "                params,\n",
    "                train_dmatrix,\n",
    "                num_boost_round=num_local_round,\n",
    "                evals=[(valid_dmatrix, \"validate\"), (train_dmatrix, \"train\")],\n",
    "            )\n",
    "            self.config = bst.save_config()\n",
    "            self.bst = bst\n",
    "        else:\n",
    "            for item in ins.parameters.tensors:\n",
    "                global_model = bytearray(item)\n",
    "\n",
    "            # Load global model into booster\n",
    "            self.bst.load_model(global_model)\n",
    "            self.bst.load_config(self.config)\n",
    "\n",
    "            bst = self._local_boost()\n",
    "\n",
    "        local_model = bst.save_raw(\"json\")\n",
    "        local_model_bytes = bytes(local_model)\n",
    "\n",
    "        return FitRes(\n",
    "            status=Status(\n",
    "                code=Code.OK,\n",
    "                message=\"OK\",\n",
    "            ),\n",
    "            parameters=Parameters(tensor_type=\"\", tensors=[local_model_bytes]),\n",
    "            num_examples=num_train,\n",
    "            metrics={},\n",
    "        )\n",
    "\n",
    "    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:\n",
    "        eval_results = self.bst.eval_set(\n",
    "            evals=[(valid_dmatrix, \"valid\")],\n",
    "            iteration=self.bst.num_boosted_rounds() - 1,\n",
    "        )\n",
    "        predictions = self.bst.predict(valid_dmatrix)\n",
    "        auc = round(float(eval_results.split(\"\\t\")[1].split(\":\")[1]), 4)\n",
    "        print(predictions)\n",
    "        # num_train, num_val                train_dmatrix  valid_dmatrix\n",
    "        # predictions = np.round(predictions)\n",
    "        # print('XGBoost model accuracy score: {0:0.4f}'. format(accuracy_score(num_val, predictions)))\n",
    "        # print(num_val)\n",
    "        # print(self.bst.get_fscore(),\"this is f1\")\n",
    "        print(self.bst.eval_set(evals=[(valid_dmatrix, \"valid\")],\n",
    "            iteration=self.bst.num_boosted_rounds() - 1,))\n",
    "        \n",
    "        return EvaluateRes(\n",
    "            status=Status(\n",
    "                code=Code.OK,\n",
    "                message=\"OK\",\n",
    "            ),\n",
    "            loss=0.0,\n",
    "            num_examples=num_val,\n",
    "            metrics={\"AUC\": auc},\n",
    "        )\n",
    "\n",
    "\n",
    "# Start Flower client\n",
    "fl.client.start_client(server_address=\"127.0.0.1:8080\", client=XgbClient().to_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xgboost.core.DMatrix object at 0x000001EA59EE6FD0>\n"
     ]
    }
   ],
   "source": [
    "train_dmatrix.feature_names\n",
    "print(train_dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13093917,  1.7058353 ,  0.5175378 , ..., -0.47649968,\n",
       "        -0.649102  , -0.74050415],\n",
       "       [-0.87608576, -1.4434521 ,  0.05769784, ..., -1.2853564 ,\n",
       "        -0.08515155,  0.39095634],\n",
       "       [ 0.12607476,  0.23739064,  1.0722127 , ...,  0.5466734 ,\n",
       "         3.48156   ,  2.5582342 ],\n",
       "       ...,\n",
       "       [-0.8540744 ,  0.3049719 , -0.40996313, ...,  0.06947917,\n",
       "        -0.17963345, -0.20951043],\n",
       "       [-0.50254136,  0.86879283, -0.22084437, ..., -0.7747355 ,\n",
       "         2.6738822 ,  3.0330305 ],\n",
       "       [-0.45625293, -1.129682  , -0.00470868, ...,  0.53176695,\n",
       "         1.586368  ,  1.44395   ]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
