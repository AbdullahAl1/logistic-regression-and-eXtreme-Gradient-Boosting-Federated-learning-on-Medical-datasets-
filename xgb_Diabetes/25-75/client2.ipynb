{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin  \\\n",
      "0         192            7      159             66              0        0   \n",
      "1         193           11      135              0              0        0   \n",
      "2         194            8       85             55             20        0   \n",
      "3         195            5      158             84             41      210   \n",
      "4         196            1      105             58              0        0   \n",
      "\n",
      "    BMI  DiabetesPedigreeFunction  Age  Outcome  \n",
      "0  30.4                     0.383   36        1  \n",
      "1  52.3                     0.578   40        1  \n",
      "2  24.4                     0.136   42        0  \n",
      "3  39.4                     0.395   29        1  \n",
      "4  24.3                     0.187   21        0  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "pima = pd.read_csv(\"./cl2.csv\")\n",
    "print(pima.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target variable (y)\n",
    "X = pima.drop(columns='Outcome')\n",
    "y = pima['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negative cases:  377 (65.45%)\n",
      "Number of positve cases:  199 (34.55%)\n"
     ]
    }
   ],
   "source": [
    "# Number of negative and positive cases in the data\n",
    "num_obs = len(pima)\n",
    "negative = len(pima.loc[pima['Outcome'] == 0])\n",
    "positive = len(pima.loc[pima['Outcome'] == 1])\n",
    "print(\"Number of negative cases:  {0} ({1:2.2f}%)\".format(negative, ((1.00 * negative)/(1.0 * num_obs)) * 100))\n",
    "print(\"Number of positve cases:  {0} ({1:2.2f}%)\".format(positive, ((1.00 * positive)/(1.0 * num_obs)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split xscale\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original negative : 377 (65.45%)\n",
      "Original positive : 199 (34.55%)\n",
      "\n",
      "Training negative : 293 (63.70%)\n",
      "Training positive : 167 (36.30%)\n",
      "\n",
      "Test negative     : 84 (72.41%)\n",
      "Test positive     : 32 (27.59%)\n"
     ]
    }
   ],
   "source": [
    "# Number of each case in the data training and testing\n",
    "print(\"Original negative : {0} ({1:0.2f}%)\".format(len(pima.loc[pima['Outcome'] == 0]), (len(pima.loc[pima['Outcome'] == 0])/len(pima.index)) * 100.0))\n",
    "print(\"Original positive : {0} ({1:0.2f}%)\".format(len(pima.loc[pima['Outcome'] == 1]), (len(pima.loc[pima['Outcome'] == 1])/len(pima.index)) * 100.0))\n",
    "print(\"\")\n",
    "print(\"Training negative : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 0]), (len(y_train[y_train[:] == 0])/len(y_train) * 100.0)))\n",
    "print(\"Training positive : {0} ({1:0.2f}%)\".format(len(y_train[y_train[:] == 1]), (len(y_train[y_train[:] == 1])/len(y_train) * 100.0)))\n",
    "print(\"\")\n",
    "print(\"Test negative     : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 0]), (len(y_test[y_test[:] == 0])/len(y_test) * 100.0)))\n",
    "print(\"Test positive     : {0} ({1:0.2f}%)\".format(len(y_test[y_test[:] == 1]), (len(y_test[y_test[:] == 1])/len(y_test) * 100.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 9) (116, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-05-13 18:08:51,185 | 1497266632.py:44 | Reformatting data...\n",
      "INFO flwr 2024-05-13 18:08:51,188 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)\n",
      "DEBUG flwr 2024-05-13 18:08:51,202 | connection.py:55 | ChannelConnectivity.IDLE\n",
      "DEBUG flwr 2024-05-13 18:08:51,205 | connection.py:55 | ChannelConnectivity.READY\n",
      "INFO flwr 2024-05-13 18:08:51,208 | 1497266632.py:100 | Start training at round 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidate-error:0.22414\ttrain-error:0.13043\n",
      "[1]\tvalidate-error:0.25000\ttrain-error:0.08478\n",
      "[2]\tvalidate-error:0.18103\ttrain-error:0.05217\n",
      "[3]\tvalidate-error:0.16379\ttrain-error:0.03261\n",
      "[4]\tvalidate-error:0.17241\ttrain-error:0.02174\n",
      "Confusion Matrix: \n",
      " [[73 11]\n",
      " [ 9 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88        84\n",
      "           1       0.68      0.72      0.70        32\n",
      "\n",
      "    accuracy                           0.83       116\n",
      "   macro avg       0.78      0.79      0.79       116\n",
      "weighted avg       0.83      0.83      0.83       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [ 8 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84        84\n",
      "           1       0.57      0.75      0.65        32\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.73      0.77      0.74       116\n",
      "weighted avg       0.80      0.78      0.78       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[67 17]\n",
      " [ 8 24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        84\n",
      "           1       0.59      0.75      0.66        32\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.74      0.77      0.75       116\n",
      "weighted avg       0.81      0.78      0.79       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[68 16]\n",
      " [ 9 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.81      0.84        84\n",
      "           1       0.59      0.72      0.65        32\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.74      0.76      0.75       116\n",
      "weighted avg       0.80      0.78      0.79       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [10 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.82        84\n",
      "           1       0.55      0.69      0.61        32\n",
      "\n",
      "    accuracy                           0.76       116\n",
      "   macro avg       0.71      0.74      0.72       116\n",
      "weighted avg       0.78      0.76      0.77       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[67 17]\n",
      " [12 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.82        84\n",
      "           1       0.54      0.62      0.58        32\n",
      "\n",
      "    accuracy                           0.75       116\n",
      "   macro avg       0.69      0.71      0.70       116\n",
      "weighted avg       0.76      0.75      0.76       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[65 19]\n",
      " [ 9 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.77      0.82        84\n",
      "           1       0.55      0.72      0.62        32\n",
      "\n",
      "    accuracy                           0.76       116\n",
      "   macro avg       0.71      0.75      0.72       116\n",
      "weighted avg       0.79      0.76      0.77       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[67 17]\n",
      " [ 9 23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.84        84\n",
      "           1       0.57      0.72      0.64        32\n",
      "\n",
      "    accuracy                           0.78       116\n",
      "   macro avg       0.73      0.76      0.74       116\n",
      "weighted avg       0.80      0.78      0.78       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [10 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.82        84\n",
      "           1       0.55      0.69      0.61        32\n",
      "\n",
      "    accuracy                           0.76       116\n",
      "   macro avg       0.71      0.74      0.72       116\n",
      "weighted avg       0.78      0.76      0.77       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[64 20]\n",
      " [10 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.76      0.81        84\n",
      "           1       0.52      0.69      0.59        32\n",
      "\n",
      "    accuracy                           0.74       116\n",
      "   macro avg       0.69      0.72      0.70       116\n",
      "weighted avg       0.77      0.74      0.75       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[65 19]\n",
      " [11 21]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.77      0.81        84\n",
      "           1       0.53      0.66      0.58        32\n",
      "\n",
      "    accuracy                           0.74       116\n",
      "   macro avg       0.69      0.72      0.70       116\n",
      "weighted avg       0.76      0.74      0.75       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [12 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.79      0.81        84\n",
      "           1       0.53      0.62      0.57        32\n",
      "\n",
      "    accuracy                           0.74       116\n",
      "   macro avg       0.69      0.71      0.69       116\n",
      "weighted avg       0.76      0.74      0.75       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[65 19]\n",
      " [12 20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.77      0.81        84\n",
      "           1       0.51      0.62      0.56        32\n",
      "\n",
      "    accuracy                           0.73       116\n",
      "   macro avg       0.68      0.70      0.69       116\n",
      "weighted avg       0.75      0.73      0.74       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [10 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.79      0.82        84\n",
      "           1       0.55      0.69      0.61        32\n",
      "\n",
      "    accuracy                           0.76       116\n",
      "   macro avg       0.71      0.74      0.72       116\n",
      "weighted avg       0.78      0.76      0.77       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[65 19]\n",
      " [13 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80        84\n",
      "           1       0.50      0.59      0.54        32\n",
      "\n",
      "    accuracy                           0.72       116\n",
      "   macro avg       0.67      0.68      0.67       116\n",
      "weighted avg       0.74      0.72      0.73       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[67 17]\n",
      " [13 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82        84\n",
      "           1       0.53      0.59      0.56        32\n",
      "\n",
      "    accuracy                           0.74       116\n",
      "   macro avg       0.68      0.70      0.69       116\n",
      "weighted avg       0.75      0.74      0.75       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [13 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81        84\n",
      "           1       0.51      0.59      0.55        32\n",
      "\n",
      "    accuracy                           0.73       116\n",
      "   macro avg       0.67      0.69      0.68       116\n",
      "weighted avg       0.75      0.73      0.74       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[67 17]\n",
      " [13 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.80      0.82        84\n",
      "           1       0.53      0.59      0.56        32\n",
      "\n",
      "    accuracy                           0.74       116\n",
      "   macro avg       0.68      0.70      0.69       116\n",
      "weighted avg       0.75      0.74      0.75       116\n",
      "\n",
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [13 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81        84\n",
      "           1       0.51      0.59      0.55        32\n",
      "\n",
      "    accuracy                           0.73       116\n",
      "   macro avg       0.67      0.69      0.68       116\n",
      "weighted avg       0.75      0.73      0.74       116\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-05-13 18:08:52,155 | connection.py:220 | gRPC channel closed\n",
      "INFO flwr 2024-05-13 18:08:52,155 | app.py:398 | Disconnect and shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[66 18]\n",
      " [13 19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.79      0.81        84\n",
      "           1       0.51      0.59      0.55        32\n",
      "\n",
      "    accuracy                           0.73       116\n",
      "   macro avg       0.67      0.69      0.68       116\n",
      "weighted avg       0.75      0.73      0.74       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "from typing import Union\n",
    "from logging import INFO\n",
    "from datasets import Dataset, DatasetDict\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import flwr as fl\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common.logger import log\n",
    "from flwr.common import (\n",
    "    Code,\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    GetParametersIns,\n",
    "    GetParametersRes,\n",
    "    Parameters,\n",
    "    Status,\n",
    ")\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# def transform_dataset_to_dmatrix(data: Union[Dataset, DatasetDict]) -> xgb.core.DMatrix:\n",
    "#     \"\"\"Transform dataset to DMatrix format for xgboost.\"\"\"\n",
    "#     x = data[\"inputs\"]\n",
    "#     y = data[\"label\"]\n",
    "#     new_data = xgb.DMatrix(x, label=y)\n",
    "#     return new_data\n",
    "\n",
    "# # Train/test splitting\n",
    "# train_data, valid_data, num_train, num_val = X_train, X_test, y_train, y_test\n",
    "num_train = 460\n",
    "num_val = 116\n",
    "\n",
    "\n",
    "xgb_train = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "xgb_test = xgb.DMatrix(X_test, y_test, enable_categorical=True)\n",
    "\n",
    "# Reformat data to DMatrix for xgboost\n",
    "log(INFO, \"Reformatting data...\")\n",
    "# train_dmatrix = transform_dataset_to_dmatrix(train_data)\n",
    "# valid_dmatrix = transform_dataset_to_dmatrix(valid_data)\n",
    "train_dmatrix = xgb_train\n",
    "valid_dmatrix = xgb_test\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Hyper-parameters for xgboost training\n",
    "num_local_round = 5\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eta\": 0.5,  # Learning rate\n",
    "    \"max_depth\": 8,\n",
    "    \"eval_metric\": \"error\",\n",
    "    \"nthread\": 16,\n",
    "    \"num_parallel_tree\": 1,\n",
    "    \"subsample\": 1,\n",
    "    \"tree_method\": \"hist\",\n",
    "}\n",
    "\n",
    "\n",
    "# Define Flower client\n",
    "class XgbClient(fl.client.Client):\n",
    "    def __init__(self):\n",
    "        self.bst = None\n",
    "        self.config = None\n",
    "\n",
    "    def get_parameters(self, ins: GetParametersIns) -> GetParametersRes:\n",
    "        _ = (self, ins)\n",
    "        return GetParametersRes(\n",
    "            status=Status(\n",
    "                code=Code.OK,\n",
    "                message=\"OK\",\n",
    "            ),\n",
    "            parameters=Parameters(tensor_type=\"\", tensors=[]),\n",
    "        )\n",
    "\n",
    "    def _local_boost(self):\n",
    "        # Update trees based on local training data.\n",
    "        for i in range(num_local_round):\n",
    "            self.bst.update(train_dmatrix, self.bst.num_boosted_rounds())\n",
    "\n",
    "        # Extract the last N=num_local_round trees for sever aggregation\n",
    "        bst = self.bst[\n",
    "            self.bst.num_boosted_rounds()\n",
    "            - num_local_round : self.bst.num_boosted_rounds()\n",
    "        ]\n",
    "\n",
    "        return bst\n",
    "\n",
    "    def fit(self, ins: FitIns) -> FitRes:\n",
    "        if not self.bst:\n",
    "            # First round local training\n",
    "            log(INFO, \"Start training at round 1\")\n",
    "            bst = xgb.train(\n",
    "                params,\n",
    "                train_dmatrix,\n",
    "                num_boost_round=num_local_round,\n",
    "                evals=[(valid_dmatrix, \"validate\"), (train_dmatrix, \"train\")],\n",
    "            )\n",
    "            self.config = bst.save_config()\n",
    "            self.bst = bst\n",
    "        else:\n",
    "            for item in ins.parameters.tensors:\n",
    "                global_model = bytearray(item)\n",
    "\n",
    "            # Load global model into booster\n",
    "            self.bst.load_model(global_model)\n",
    "            self.bst.load_config(self.config)\n",
    "\n",
    "            bst = self._local_boost()\n",
    "\n",
    "        local_model = bst.save_raw(\"json\")\n",
    "        local_model_bytes = bytes(local_model)\n",
    "\n",
    "        return FitRes(\n",
    "            status=Status(\n",
    "                code=Code.OK,\n",
    "                message=\"OK\",\n",
    "            ),\n",
    "            parameters=Parameters(tensor_type=\"\", tensors=[local_model_bytes]),\n",
    "            num_examples=num_train,\n",
    "            metrics={},\n",
    "        )\n",
    "\n",
    "    def evaluate(self, ins: EvaluateIns) -> EvaluateRes:\n",
    "        eval_results = self.bst.eval_set(\n",
    "            evals=[(valid_dmatrix, \"valid\")],\n",
    "            iteration=self.bst.num_boosted_rounds() - 1,\n",
    "        )\n",
    "        auc = round(float(eval_results.split(\"\\t\")[1].split(\":\")[1]), 4)\n",
    "        \n",
    "        preds = self.bst.predict(valid_dmatrix)\n",
    "        y_pred = np.multiply(preds,100)\n",
    "        y_pred = y_pred.astype(int)\n",
    "        a = [1 if i >= 50 else 0 for i in y_pred]\n",
    "        print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, a))\n",
    "        print(metrics.classification_report(y_test,a))\n",
    "        \n",
    "\n",
    "        return EvaluateRes(\n",
    "            status=Status(\n",
    "                code=Code.OK,\n",
    "                message=\"OK\",\n",
    "            ),\n",
    "            loss=0.0,\n",
    "            num_examples=num_val,\n",
    "            metrics={\"error\": auc},\n",
    "        )\n",
    "\n",
    "\n",
    "# Start Flower client\n",
    "fl.client.start_client(server_address=\"127.0.0.1:8080\", client=XgbClient().to_client())\n",
    "\n",
    "# Confusion Matrix: \n",
    "#  [[69 15]\n",
    "#  [ 7 25]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.91      0.82      0.86        84\n",
    "#            1       0.62      0.78      0.69        32\n",
    "\n",
    "#     accuracy                           0.81       116\n",
    "#    macro avg       0.77      0.80      0.78       116\n",
    "# weighted avg       0.83      0.81      0.82       116\n",
    "\n",
    "# 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix: \n",
      " [[62 22]\n",
      " [10 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.79        84\n",
      "           1       0.50      0.69      0.58        32\n",
      "\n",
      "    accuracy                           0.72       116\n",
      "   macro avg       0.68      0.71      0.69       116\n",
      "weighted avg       0.76      0.72      0.74       116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier \n",
    "\n",
    "xgb = XGBClassifier(objective = 'binary:logistic')\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "print()\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
